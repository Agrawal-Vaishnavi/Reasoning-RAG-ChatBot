{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Unp50nVmF-Nf",
        "lVyn9u33_e2e",
        "GDH-xV-9_rsq",
        "kMBHzvyLoiDq",
        "NQEQOCxK_whU",
        "2rWk0d3PjdRl",
        "Wpntmfrn_5nL",
        "gYbG26tVHLE_",
        "5cC38PQVF3L7",
        "A51dDBR0Fskr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }   
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reasoning Enhanced RAG System\n",
        "\n",
        "A chatbot that combines retrieval-augmented generation with chain-of-thought reasoning"
      ],
      "metadata": {
        "id": "D-XnpN-JL8Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installation and Dependencies"
      ],
      "metadata": {
        "id": "Unp50nVmF-Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWGuX79vMqTg",
        "outputId": "e5af06cc-2d15-4027-f5c7-2ae77774273e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "import faiss\n",
        "import json\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import glob  # For finding files matching a pattern"
      ],
      "metadata": {
        "id": "IevLVbNoNCtr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "d1lZcJxiNIqe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DocumentStore: Manages document embeddings and retrieval\n",
        "\n",
        "Uses SentenceTransformer for embedding documents\n",
        "\n",
        "Implements FAISS for efficient vector similarity search"
      ],
      "metadata": {
        "id": "lVyn9u33_e2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentStore:\n",
        "    \"\"\"Store and retrieve documents with vector embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"Initialize the document store with an embedding model\"\"\"\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.documents = []\n",
        "        self.document_embeddings = None\n",
        "        self.index = None\n",
        "\n",
        "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
        "        \"\"\"Add documents to the store and update index\"\"\"\n",
        "        self.documents.extend(documents)\n",
        "\n",
        "        # Extract text for embedding\n",
        "        texts = [doc[\"text\"] for doc in documents]\n",
        "\n",
        "        # Generate embeddings\n",
        "        new_embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        if self.document_embeddings is None:\n",
        "            self.document_embeddings = new_embeddings\n",
        "        else:\n",
        "            self.document_embeddings = np.vstack([self.document_embeddings, new_embeddings])\n",
        "\n",
        "        # Build or update FAISS index\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
        "        vector_dimension = self.document_embeddings.shape[1]      #second dimension which represents the length of each embedding vector (384 for used model)\n",
        "        self.index = faiss.IndexFlatL2(vector_dimension)          #creates a new index using L2 (Euclidean) distance\n",
        "        self.index.add(self.document_embeddings)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for documents similar to the query\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search the index\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return the top k documents\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.documents):\n",
        "                doc = self.documents[idx].copy()\n",
        "                doc[\"score\"] = float(distances[0][i])\n",
        "                results.append(doc)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "uhQcb6YINQfU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ReasoningModule: Generates chain-of-thought reasoning\n",
        "\n",
        "Uses a language model to analyze context\n",
        "\n",
        "Produces step-by-step reasoning about retrieved documents"
      ],
      "metadata": {
        "id": "GDH-xV-9_rsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reasoning Module with flan-t5-base"
      ],
      "metadata": {
        "id": "kMBHzvyLoiDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningModule:\n",
        "    \"\"\"Module for generating chain-of-thought reasoning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-base\"):\n",
        "        \"\"\"Initialize with a reasoning model\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    def generate_reasoning(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate reasoning steps for a query given context\"\"\"\n",
        "        # Prepare reasoning prompt\n",
        "        prompt = self._create_reasoning_prompt(query, context)\n",
        "\n",
        "        # Generate reasoning\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        reasoning = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return reasoning\n",
        "\n",
        "    def _create_reasoning_prompt(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Create a prompt for the reasoning model\"\"\"\n",
        "        context_str = \"\\n\\n\".join([f\"Document {i+1}: {doc['text']}\" for i, doc in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Given the following context information and question, reason step by step to find the answer.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Let's think about this step by step:\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "Ffd8jowqNXau"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RAGReasoner: Orchestrates the entire pipeline\n",
        "\n",
        "Initial document retrieval\n",
        "\n",
        "Reasoning generation\n",
        "\n",
        "Query refinement based on reasoning\n",
        "\n",
        "Final document retrieval with refined query\n",
        "\n",
        "Answer generation"
      ],
      "metadata": {
        "id": "NQEQOCxK_whU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RAGReasoner with Flan-T5-Base"
      ],
      "metadata": {
        "id": "2rWk0d3PjdRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGReasoner:\n",
        "    \"\"\"Main class that combines retrieval and reasoning\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        document_store: DocumentStore,\n",
        "        reasoning_module: ReasoningModule,\n",
        "        model_name: str = \"google/flan-t5-base\",\n",
        "        retrieval_k: int = 5\n",
        "    ):\n",
        "        \"\"\"Initialize with components and parameters\"\"\"\n",
        "        self.document_store = document_store\n",
        "        self.reasoning_module = reasoning_module\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        self.retrieval_k = retrieval_k                                    #number of documents to retrieve during the search process\n",
        "\n",
        "    def process_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query through the entire pipeline\"\"\"\n",
        "        # 1. Initial document retrieval\n",
        "        initial_docs = self.document_store.search(query, self.retrieval_k)\n",
        "\n",
        "        # 2. Generate reasoning based on retrieved documents\n",
        "        reasoning = self.reasoning_module.generate_reasoning(query, initial_docs)\n",
        "\n",
        "        # 3. Use reasoning to refine the query\n",
        "        refined_query = self._refine_query(query, reasoning)\n",
        "\n",
        "        # 4. Retrieve documents again with the refined query\n",
        "        refined_docs = self.document_store.search(refined_query, self.retrieval_k)\n",
        "\n",
        "        # 5. Generate the final answer\n",
        "        answer = self._generate_answer(query, reasoning, refined_docs)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"refined_query\": refined_query,\n",
        "            \"reasoning\": reasoning,\n",
        "            \"initial_docs\": initial_docs,\n",
        "            \"refined_docs\": refined_docs,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "    def _refine_query(self, original_query: str, reasoning: str) -> str:\n",
        "        \"\"\"Refine the query based on reasoning\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "Original query: {original_query}\n",
        "\n",
        "Reasoning process:\n",
        "{reasoning}\n",
        "\n",
        "Based on this reasoning, provide a refined and expanded search query that would better retrieve relevant information:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        refined_query = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return refined_query\n",
        "\n",
        "    def _generate_answer(self, query: str, reasoning: str, documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate final answer based on query, reasoning and documents\"\"\"\n",
        "        # Create context string from documents\n",
        "        context_str = \"\\n\\n\".join([f\"Document {i+1}: {doc['text']}\" for i, doc in enumerate(documents)])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Query: {query}\n",
        "\n",
        "Reasoning process:\n",
        "{reasoning}\n",
        "\n",
        "Retrieved documents:\n",
        "{context_str}\n",
        "\n",
        "Based on the reasoning and documents, provide a comprehensive and accurate answer to the query:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "OYriQGvONeGr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ReasoningRAGChatbot: Provides the chat interface\n",
        "\n",
        "Tracks conversation history\n",
        "\n",
        "Handles user inputs\n",
        "\n",
        "Returns responses or detailed results"
      ],
      "metadata": {
        "id": "Wpntmfrn_5nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningRAGChatbot:\n",
        "    \"\"\"Chatbot interface for the RAG Reasoning system\"\"\"\n",
        "\n",
        "    def __init__(self, rag_reasoner: RAGReasoner):\n",
        "        \"\"\"Initialize with RAGReasoner\"\"\"\n",
        "        self.rag_reasoner = rag_reasoner\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        \"\"\"Process user input and generate a response\"\"\"\n",
        "        # Add user input to conversation history\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Process the query\n",
        "        result = self.rag_reasoner.process_query(user_input)\n",
        "\n",
        "        # Format a response message\n",
        "        response = f\"Answer: {result['answer']}\"\n",
        "\n",
        "        # Add response to conversation history\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return response\n",
        "\n",
        "    def get_detailed_response(self, user_input: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process user input and return detailed results including reasoning\"\"\"\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        result = self.rag_reasoner.process_query(user_input)\n",
        "\n",
        "        response = f\"Answer: {result['answer']}\"\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return result\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "\n"
      ],
      "metadata": {
        "id": "u1AEsgqmNhrd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare Dataset"
      ],
      "metadata": {
        "id": "gYbG26tVHLE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    \"\"\"Utility class for loading datasets\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_json(filepath: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load documents from a JSON file\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def create_documents_from_texts(texts: List[str], metadata: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Create document objects from text strings\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            doc = {\"id\": i, \"text\": text}\n",
        "\n",
        "            if metadata and i < len(metadata):\n",
        "                doc.update(metadata[i])\n",
        "\n",
        "            documents.append(doc)\n",
        "\n",
        "        return documents\n"
      ],
      "metadata": {
        "id": "uugjkhB0Nkul"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "5cC38PQVF3L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluation:\n",
        "    \"\"\"Evaluation metrics for the RAG system\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_retrieval(retrieved_docs: List[Dict[str, Any]], relevant_ids: List[int]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate retrieval performance\"\"\"\n",
        "        retrieved_ids = [doc[\"id\"] for doc in retrieved_docs]\n",
        "\n",
        "        # Calculate precision\n",
        "        if not retrieved_ids:\n",
        "            precision = 0.0\n",
        "        else:\n",
        "            precision = len(set(retrieved_ids) & set(relevant_ids)) / len(retrieved_ids)\n",
        "\n",
        "        # Calculate recall\n",
        "        if not relevant_ids:\n",
        "            recall = 1.0\n",
        "        else:\n",
        "            recall = len(set(retrieved_ids) & set(relevant_ids)) / len(relevant_ids)\n",
        "\n",
        "        # Calculate F1\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4DE_c4jeNoZK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main function for Sample texts"
      ],
      "metadata": {
        "id": "A51dDBR0Fskr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the system\"\"\"\n",
        "    # Sample documents\n",
        "    sample_texts = [\"OpenAI's GPT-4 is a multimodal large language model capable of understanding both text and image inputs.\",\n",
        "                    \"Retrieval-Augmented Generation (RAG) enhances chatbot performance by fetching relevant external information in real-time before generating a response.\",\n",
        "                    \"Vector databases like FAISS, Pinecone, and Weaviate are commonly used for storing and retrieving document embeddings in RAG pipelines.\",\n",
        "                    \"Fine-tuning or prompt engineering can help tailor the responses of LLMs to domain-specific use cases, such as legal, healthcare, or customer support.\",\n",
        "                    \"Embedding models like text-embedding-ada-002 from OpenAI or all-MiniLM-L6-v2 from Sentence Transformers are popular for generating vector representations of text.\",\n",
        "                    \"LangChain and LlamaIndex are popular frameworks for building RAG-based applications and connecting LLMs with knowledge sources.\",\n",
        "                    \"Chunking strategies, such as splitting by sentence or paragraph, play a critical role in retrieval accuracy and context relevance.\",\n",
        "                    \"RAG systems can outperform standard chatbots in enterprise environments by reducing hallucinations and increasing factual accuracy.\",\n",
        "                    \"Prompt engineering involves crafting inputs to guide language models toward more accurate, useful, and reliable outputs.\",\n",
        "                    \"Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems.\"\n",
        "                  ]\n",
        "\n",
        "    # Create documents\n",
        "    documents = Dataset.create_documents_from_texts(sample_texts)\n",
        "\n",
        "    # Initialize components\n",
        "    document_store = DocumentStore()\n",
        "    document_store.add_documents(documents)\n",
        "\n",
        "    reasoning_module = ReasoningModule()\n",
        "\n",
        "    rag_reasoner = RAGReasoner(document_store, reasoning_module)\n",
        "\n",
        "    chatbot = ReasoningRAGChatbot(rag_reasoner)\n",
        "\n",
        "    # Demo query\n",
        "    query = \"How does RAG relate to NLP?\"\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    result = chatbot.get_detailed_response(query)\n",
        "    print(\"\\nDetailed Response:\")\n",
        "    print(f\"Reasoning: {result['reasoning']}\")\n",
        "    print(f\"Refined Query: {result['refined_query']}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "    # Simple chat interface\n",
        "    print(\"\\n--- Simple Chat Interface ---\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "EJBxzpD0NtF9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c38f92bef8e34de69f9bb3af845d2a6b",
            "a1653c04626046ae84b2ddf5847544b2",
            "db8639e75fcb429db5fdd99571e995d9",
            "2ca49f0c39f8403ab206f60c377d0f1e",
            "6d40df4fc5f5464abb1bf60c0045331d",
            "2e59c9a51931456586485ca19aeb6e8d",
            "709f79fe112a40478240081b10a4973e",
            "d0349ee51c8f4110a9e8a43cbd5449fa",
            "552e7bb4456b4aa688f28056942ab1bd",
            "2bfca58323244021b6454ecc64dcd23a",
            "9836b51f415f4cc592b472d4a44017de",
            "f589b5d2cd1145afa1f419beeb683381",
            "fc82656759b249a081a89fd0fdc30d51",
            "a4c73e719e9c4fce8f059d4aec6db3ea",
            "12980aca64ef4b1786e0c88e786c9bb0",
            "8858cb20e02f4e85afca255835354dd2",
            "fabe87b1a30a49cc9cf304b2deb44db7",
            "6273d1274afa40a49b7e29f98cee89cd",
            "fbe6d30258834e3cb43c25c807ec7ceb",
            "285187cd91f842c0a83890046cbd34ec",
            "6573c1b5f74244dcab447296c22b7ce8",
            "9abd9d91a9b8402faa3fdf0f61d125aa",
            "85a92bdd86cd43cbabce0ca098e6f361",
            "71ce65dd68fe48169cd7326ad7725ac3",
            "a23fa8ce46da4a829cfa162f75cfbaaf",
            "fa92174f980841148fb1045ef23451f5",
            "b8938bb90b85401cb3fbd4101e2ff2c9",
            "c2430c8aae294dbdb68c278de9a949b9",
            "a48bec55165844f8982f82af78345dff",
            "f5f4cf50f65e4be88e03c6ed0b9d92a0",
            "238fffd8e09347c8ba5095e2ec586bbf",
            "b39d9afe1596406f897f2a9c1d9e8faf",
            "a0826f1d0d1c419fbbede34758553043",
            "d9e8f54ebc60447cb8aebe5de3945a74",
            "ba3ea36b73ee475fa821ff5d7051a2eb",
            "db4659a3831d40a584207c5a8e452aaf",
            "4261833c77164eddbf8fff162069e0e2",
            "07f951b9467746a89b106cf8481ca377",
            "d7750871a2634cc58991a55e8c6a9a9d",
            "311a80a8c6bc4ed39ec984c1fe136742",
            "accd0eed50c44dfcb6f0ac98f5bfceac",
            "cd56d3b6db194487944d32445a066345",
            "6b9fd2772bb7415ba7abde711a493f86",
            "c8760e9a8b78406ab421d866cb640194",
            "98d619689e6f49ae9911a569352fa076",
            "5dd089bca9c64b1c8d1f7716088eb5bf",
            "e1618ae17a014a0bbc2cd9ccd9bbd914",
            "6d9e7417db444bdbac05d7b4cd595d23",
            "aebf0b06e5db4fa988207c0fd4351af2",
            "509f70b17f814236b7c657775464ce05",
            "aede2d69a0264527888c06220d00b129",
            "d0614f48557246a29362acb4c22e0563",
            "890639c0ffb541a1b25a10c8c50f2015",
            "b606efddd285465a82bcf90facd10f94",
            "20a82f2b6dd04a909bee66b2f1d2dde6",
            "2cd17bf4116b488da5ab742b6fedb421",
            "6771dbcd920c4e51ae7d147c9ef986b7",
            "5e707538646f433a92d53fbb40a14cdd",
            "8431a8ccaf1d46e3b7d9ee9cd6d6212e",
            "61c22635d68945819471480f5301483d",
            "1861d8e35ac44daba75d8748c96019e0",
            "0474c01e39f648d3866b7d866b36cb22",
            "6d36e91a4bec4ffaba3b477e702ffb6f",
            "23f881d415fd4b8097f253564607c144",
            "c093bfa1af014d04a78d6913017bac12",
            "d49e94d45b1b4100b472bbd699450acb",
            "b59bb601228843d9bc626d826d32e675",
            "3d31d12a626b45f5830bc0eadd315478",
            "08278940ce294b87a16fdc47e94148b0",
            "961151a757f64aef8d9b890ef25c1a7d",
            "f630e65ebaa94569906f084717fd489a",
            "f14d1a0c77204e208838aaa8419417ee",
            "04d0e1f34fa043e2a113bba7c00e2d84",
            "f290ed649ca34ca1857412fdd5ef21f8",
            "eed421900ffa4f8bb3de0d56b2f733e0",
            "0eb4b5fef06a46048e10ffef1d6e6017",
            "6fd358a65ca64c7589abf032286b44ad",
            "7379bb9c015548b0966175236a94a699",
            "7aad61ddd9cf4342ad424a6d0256fd14",
            "179c11e778194db4b5df8d4d4f30ff6e",
            "861dde83b4e24dad895a21ab32556ed4",
            "5420a89b0069489484665fbe1c70d0ad",
            "518c901829784d209cd3b77a4e19512c",
            "f74d580ef0d245c2ba048c50f5dc07a5",
            "27425c063b2e49258c82a781c86ad068",
            "13002a5507c94db5a7071b46c8d29726",
            "23b9f34ff9f84c988b12d910362601df",
            "1d9d24087fe549b3835f0f513446c242",
            "b7a6abd6754a407792def740f20ef61a",
            "2b43fbbe2607453dab53d42635593194",
            "b3815af3d57d4da0b6e037fa33c76349",
            "c8a273614c604d0a89d4ad40e51b0abd",
            "170e7a6234524720b22d9f082d83beaf",
            "22310aaa44f54c56bb430a67b419f16e",
            "4e95d3ee567c4cfca061c13c452509a2",
            "4bce797a2686489b97e9c3e5f19d1b58",
            "b04019a4a612476eaaac92c98d3c4244",
            "1c0ce58333c64fe381571f57f2db5f50",
            "1e89138d634a44039226e55359470869",
            "c1b1cf87c15346bdbfb6e61fa5f8b6df",
            "e9423496ebc146b786cf7865f0da654a",
            "8db38a44843843e099942effc17416b3",
            "bc3494ec26b84a2a9908b200c3154e07",
            "999b773a859e49dfa487dc35f98e7842",
            "7f89363a07db4e42bbbcdad333befa4c",
            "d1f845ac150044ac8c2395d02125f8be",
            "a63cae5dfe4e462a94aa0b3796765319",
            "12440b17db8c41dfaf9c570c66539bd9",
            "8b3427fcc5d94952b599fef1cb03b0a4",
            "dc512c062363417da507ccffbadb7a22",
            "9c41660dbdef413e9d170d1cfcfcc29c",
            "34a4156061a945aeb608dfe13493cb9d",
            "f400dfe5419e450dab26a1c78ed484bc",
            "07a8289cc8bd400188eed24d8c0dac9f",
            "cbe32e9f497041ccaff435af5ef86fed",
            "15e99688046b4a4ab944b19a6cdf718c",
            "1a576796917440f0b270c7f993507b37",
            "804733d187c74c9ca5afc6f300389899",
            "ec35a0718cec45ec8bdc635f96ca2baa",
            "fd558584a42f43edba72fa9fe45f20a0",
            "a33191bcd234486c8b2fce14c8ef649c",
            "4a9c87983676409793835d3aa5d680bd",
            "001f926d194c4a8fbb4d7737ddb03447",
            "400cc94c06d049bfb1840d5d1a389513",
            "c7d1560f271e4921a488578459c36f41",
            "6d66be23a9664c708d7a38b663438b3e",
            "568001f66b32412ab65e952b8436069e",
            "33ea606285c84f23beb09a65ef88a5b5",
            "e3fa01f9bce0483a88e31a7b27311f47",
            "eaaf387fba0d4eb6b865ebcee211eec8",
            "c71f84ec4340485e8a693f3249d5c733",
            "d0d2391c4e2e42ea81b25a05f43f3fb7",
            "46376a8958a8482797fe4b08bd31dd54",
            "24ad137a9ad4408a83b1d0c30405da2e",
            "a1d57433de59489e93737fb21758fa35",
            "68208bda0fc64fa8ae6dfca1c4b3d58a",
            "400144c3c69244188a7d42cf571dc7e6",
            "a5ab829236d448e9b7ca83c4b36f1547",
            "b37c65aa447745a19f1632ae33630382",
            "5fccec41390644d3b8be3b308b01cf36",
            "b1fb5e116c3743b0a284759451fa93a8",
            "c246974f6fde46cd9dbacce7a7d19777",
            "646702f63fab4ed7b68c97c9c8df2e27",
            "8c80ec899512421db6bf2fd723847868",
            "fbb38d4334b548f6ac02195637b70869",
            "58c98201a2f84b028c356f21c8afb1da",
            "32b72b3c62f146bc9a624e3a62a1b83f",
            "f918796ef67b4f9881d08e2bb7e6972c",
            "c4aa1e0e006a4a04abd9a9dc04aa6dd6",
            "7d47c48cec194d4597d565d694e5e777",
            "c9666989d90245e2ab0bec41b154f7b3",
            "0dee0fe1c3044749afd733544145de21",
            "3fc8944efd2a48c8896f823481da6e23",
            "82a38fdd7252461eb0fae4cc19a7ae4a",
            "ce093bf063794794b9324e88e0e09c4c",
            "a69eb6106c364b57a819434915dec2fc",
            "6c7b720872c047c59e9c96add6731d93",
            "e2a7266fb6a84b9192a026f93cb0fae8",
            "36f1b8abed4849ff91dc4e8dab26af9a",
            "a41fc6bc965c4ab1a9a852384e5847ed",
            "00f6ab3295154aa58898a5203360e1e4",
            "8429f20b1e4441e48bb4dd1555b71854",
            "b4e2a99ba06b4d8d989bb4b928f26e37",
            "357b6c0859e94104a90ff325b66b7e18",
            "869cf2f5f08243d5a04194bb89f7c505",
            "25e3f1ee70214e80982d9b1431bf4a4a",
            "1c95c2ae126544f183cec93c2a7fc022",
            "07a37ecd41bc4ec986f8c5842cfbb4e6",
            "241787f01bb64b7a8ec98248478bb8e2",
            "ee54995a0b00404682538c859a2c4477",
            "92bd504cdac54f0291a33a514426b9d7",
            "dfd94376e9954213b9fa0a0d96b58ce7",
            "f48489deac9f4a8480ca4e32309e9ed4",
            "5cc01dd788c04498ae36a83050290d1b",
            "fc6764b5d0284549a609ffb0646ebe41",
            "f596139c318647d6bfeef69bfa4c9d1d",
            "033d97a32c7f4d3ca70ff1464f142c27",
            "07fbe5e6bf314d818585776588fde9c2",
            "0f2c7e1c9fd046e88baaeeeeade0fe32",
            "55daea51b2164442aaf7eaa823aa9236",
            "630289003a58447db911f6c55e9bdd5d",
            "63165d600ed24be08acc9e15446ef99e",
            "d8696f8fc598458f833f1d9515b17e75",
            "7343d78f5c494440915719278d791adc",
            "06ea54d57eac4dfb8352ce7ca6c3498c",
            "029ce68779e843b8b2f452192c169098",
            "3957469eb11c48709cd0f8b470b901c4",
            "8803424c5d1240ccb918c5807504fbd0",
            "201f47eb59954c5fbb9bee9245664c56",
            "2879e9d66a0f4942bcfe71f7f272be26",
            "b72a4ceb8c8e4413a0cf7471d06ab9bb",
            "f97b079ce271419492ffb229dcf0b3b4",
            "0c52fb413328424eaeb0415a2b4157bb",
            "7ad6644bd5f54a2f86be7ecac64ddc12",
            "2f143ab6381c422dbe380a05663ca5c8",
            "6ae6b95811304428b1f84f68a625fa7c",
            "7e584807c68f4020b90228a147a7a9c2",
            "b4b4649246174c389d282d56f3da8086",
            "863504d35ced44d8a86dc535f1f06f24",
            "414c64dc69b041ce960cd96ed3822380",
            "0b8abdd7cf4d435b99d11fc335338524",
            "5f02759014e74feca3ddd23c68949ade",
            "ce3e3b1550ed40d29ac7308cad6c8840",
            "f0b1244258a5408f89ca00d658cfda4a",
            "aa62c13514894faca58d3b79637698e7",
            "538a495abfd3490288cc854d2600cd61",
            "b14e95d413244bed9b796aa2e7a8d950",
            "3c92697d0dee48beaec850b0091f88ab",
            "208c780696dc496ab7a2ff18daab4078"
          ]
        },
        "id": "zbF2kaizNwJz",
        "outputId": "e24ef6b9-aca5-488d-cd5e-2ebf328a4ce7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c38f92bef8e34de69f9bb3af845d2a6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f589b5d2cd1145afa1f419beeb683381"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85a92bdd86cd43cbabce0ca098e6f361"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9e8f54ebc60447cb8aebe5de3945a74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d619689e6f49ae9911a569352fa076"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cd17bf4116b488da5ab742b6fedb421"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59bb601228843d9bc626d826d32e675"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7379bb9c015548b0966175236a94a699"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7a6abd6754a407792def740f20ef61a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1b1cf87c15346bdbfb6e61fa5f8b6df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c41660dbdef413e9d170d1cfcfcc29c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a9c87983676409793835d3aa5d680bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46376a8958a8482797fe4b08bd31dd54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c80ec899512421db6bf2fd723847868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce093bf063794794b9324e88e0e09c4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e3f1ee70214e80982d9b1431bf4a4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "033d97a32c7f4d3ca70ff1464f142c27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8803424c5d1240ccb918c5807504fbd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "863504d35ced44d8a86dc535f1f06f24"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How does RAG relate to NLP?\n",
            "\n",
            "Detailed Response:\n",
            "Reasoning: Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems. Document 3: Vector databases like FAISS, Pinecone, and Weaviate are commonly used for storing and retrieving document embeddings in RAG pipelines. Document 4: RAG systems can outperform standard chatbots in enterprise environments by reducing hallucinations and increasing factual accuracy. Document 5: Chunking strategies, such as splitting by sentence or paragraph, play a critical role in retrieval accuracy and context relevance. Therefore, the final answer is document pre-processing.\n",
            "Refined Query: Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems\n",
            "Answer: document pre-processing\n",
            "\n",
            "--- Simple Chat Interface ---\n",
            "\n",
            "You: What is the role of RAG?\n",
            "Bot: Answer: Enhances chatbot performance by fetching relevant external information in real-time before generating a response.\n",
            "\n",
            "You: What is the benefit of vector database?\n",
            "Bot: Answer: document embeddings in RAG pipelines\n",
            "\n",
            "You: What is LangChain popular for?\n",
            "Bot: Answer: building RAG - based applications and connecting LLMs with knowledge sources\n",
            "\n",
            "You: What is prompt engineering?\n",
            "Bot: Answer: Prompt engineering involves crafting inputs to guide language models toward more accurate, useful, and reliable outputs.\n",
            "\n",
            "You: What are foundational components of building RAG?\n",
            "Bot: Answer: Document pre-processing, metadata tagging, and semantic search\n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vDSYQjViTAtn"
      }
    }
  ]
}