{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Unp50nVmF-Nf",
        "lVyn9u33_e2e",
        "GDH-xV-9_rsq",
        "kMBHzvyLoiDq",
        "NQEQOCxK_whU",
        "2rWk0d3PjdRl",
        "Wpntmfrn_5nL",
        "gYbG26tVHLE_",
        "5cC38PQVF3L7",
        "A51dDBR0Fskr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97cb02ccc66d4bac9db08ffd1bce4b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5169979d2614488185f65a7ed4cf080a",
              "IPY_MODEL_417109c358224cf9932027c834237279",
              "IPY_MODEL_3c8b1f93a65043c986a06797116d0aeb"
            ],
            "layout": "IPY_MODEL_ddddd1bf798e44afba4dc3b0989099d7"
          }
        },
        "5169979d2614488185f65a7ed4cf080a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bca6b41d73584523befb8f3ecd70a1fd",
            "placeholder": "​",
            "style": "IPY_MODEL_3c221ceb1ded4fa09cbda7c526ee99b5",
            "value": "Batches: 100%"
          }
        },
        "417109c358224cf9932027c834237279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcdc74164d1746b1a110a6fff09bb218",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a48b79b94f324275b9335304fe89cf8d",
            "value": 1
          }
        },
        "3c8b1f93a65043c986a06797116d0aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a580f40b118143e1977dd5c226a29bb9",
            "placeholder": "​",
            "style": "IPY_MODEL_b9b5698b0f3346a686e278753d870b57",
            "value": " 1/1 [00:00&lt;00:00,  5.06it/s]"
          }
        },
        "ddddd1bf798e44afba4dc3b0989099d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bca6b41d73584523befb8f3ecd70a1fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c221ceb1ded4fa09cbda7c526ee99b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcdc74164d1746b1a110a6fff09bb218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48b79b94f324275b9335304fe89cf8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a580f40b118143e1977dd5c226a29bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b5698b0f3346a686e278753d870b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reasoning Enhanced RAG System\n",
        "\n",
        "A chatbot that combines retrieval-augmented generation with chain-of-thought reasoning"
      ],
      "metadata": {
        "id": "D-XnpN-JL8Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installation and Dependencies"
      ],
      "metadata": {
        "id": "Unp50nVmF-Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWGuX79vMqTg",
        "outputId": "18d7fe92-11c0-4b86-f657-f1d8b84c4cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RweNJLgcbqQ",
        "outputId": "f90c72ed-0b34-40bc-990c-e995a91af1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "import faiss\n",
        "import json\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import glob  # For finding files matching a pattern"
      ],
      "metadata": {
        "id": "IevLVbNoNCtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "d1lZcJxiNIqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DocumentStore: Manages document embeddings and retrieval\n",
        "\n",
        "Uses SentenceTransformer for embedding documents\n",
        "\n",
        "Implements FAISS for efficient vector similarity search"
      ],
      "metadata": {
        "id": "lVyn9u33_e2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentStore:\n",
        "    \"\"\"Store and retrieve documents with vector embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"Initialize the document store with an embedding model\"\"\"\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "        self.documents = []\n",
        "        self.document_embeddings = None\n",
        "        self.index = None\n",
        "\n",
        "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
        "        \"\"\"Add documents to the store and update index\"\"\"\n",
        "        self.documents.extend(documents)\n",
        "\n",
        "        # Extract text for embedding\n",
        "        texts = [doc[\"text\"] for doc in documents]\n",
        "\n",
        "        # Generate embeddings\n",
        "        new_embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        if self.document_embeddings is None:\n",
        "            self.document_embeddings = new_embeddings\n",
        "        else:\n",
        "            self.document_embeddings = np.vstack([self.document_embeddings, new_embeddings])\n",
        "\n",
        "        # Build or update FAISS index\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"Build FAISS index for fast similarity search\"\"\"\n",
        "        vector_dimension = self.document_embeddings.shape[1]      #second dimension which represents the length of each embedding vector (384 for used model)\n",
        "        self.index = faiss.IndexFlatL2(vector_dimension)          #creates a new index using L2 (Euclidean) distance\n",
        "        self.index.add(self.document_embeddings)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for documents similar to the query\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search the index\n",
        "        distances, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return the top k documents\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.documents):\n",
        "                doc = self.documents[idx].copy()\n",
        "                doc[\"score\"] = float(distances[0][i])\n",
        "                results.append(doc)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "uhQcb6YINQfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ReasoningModule: Generates chain-of-thought reasoning\n",
        "\n",
        "Uses a language model to analyze context\n",
        "\n",
        "Produces step-by-step reasoning about retrieved documents"
      ],
      "metadata": {
        "id": "GDH-xV-9_rsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reasoning Module with flan-t5-base"
      ],
      "metadata": {
        "id": "kMBHzvyLoiDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningModule:\n",
        "    \"\"\"Module for generating chain-of-thought reasoning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-base\"):\n",
        "        \"\"\"Initialize with a reasoning model\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    def generate_reasoning(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate reasoning steps for a query given context\"\"\"\n",
        "        # Prepare reasoning prompt\n",
        "        prompt = self._create_reasoning_prompt(query, context)\n",
        "\n",
        "        # Generate reasoning\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        reasoning = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return reasoning\n",
        "\n",
        "    def _create_reasoning_prompt(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Create a prompt for the reasoning model\"\"\"\n",
        "        context_str = \"\\n\\n\".join([f\"Document {i+1}: {doc['text']}\" for i, doc in enumerate(context)])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Given the following context information and question, reason step by step to find the answer.\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Let's think about this step by step:\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "Ffd8jowqNXau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RAGReasoner: Orchestrates the entire pipeline\n",
        "\n",
        "Initial document retrieval\n",
        "\n",
        "Reasoning generation\n",
        "\n",
        "Query refinement based on reasoning\n",
        "\n",
        "Final document retrieval with refined query\n",
        "\n",
        "Answer generation"
      ],
      "metadata": {
        "id": "NQEQOCxK_whU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RAGReasoner with Flan-T5-Base"
      ],
      "metadata": {
        "id": "2rWk0d3PjdRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGReasoner:\n",
        "    \"\"\"Main class that combines retrieval and reasoning\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        document_store: DocumentStore,\n",
        "        reasoning_module: ReasoningModule,\n",
        "        model_name: str = \"google/flan-t5-base\",\n",
        "        retrieval_k: int = 5\n",
        "    ):\n",
        "        \"\"\"Initialize with components and parameters\"\"\"\n",
        "        self.document_store = document_store\n",
        "        self.reasoning_module = reasoning_module\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        self.retrieval_k = retrieval_k                                    #number of documents to retrieve during the search process\n",
        "\n",
        "    def process_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query through the entire pipeline\"\"\"\n",
        "        # 1. Initial document retrieval\n",
        "        initial_docs = self.document_store.search(query, self.retrieval_k)\n",
        "\n",
        "        # 2. Generate reasoning based on retrieved documents\n",
        "        reasoning = self.reasoning_module.generate_reasoning(query, initial_docs)\n",
        "\n",
        "        # 3. Use reasoning to refine the query\n",
        "        refined_query = self._refine_query(query, reasoning)\n",
        "\n",
        "        # 4. Retrieve documents again with the refined query\n",
        "        refined_docs = self.document_store.search(refined_query, self.retrieval_k)\n",
        "\n",
        "        # 5. Generate the final answer\n",
        "        answer = self._generate_answer(query, reasoning, refined_docs)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"refined_query\": refined_query,\n",
        "            \"reasoning\": reasoning,\n",
        "            \"initial_docs\": initial_docs,\n",
        "            \"refined_docs\": refined_docs,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "    def _refine_query(self, original_query: str, reasoning: str) -> str:\n",
        "        \"\"\"Refine the query based on reasoning\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "Original query: {original_query}\n",
        "\n",
        "Reasoning process:\n",
        "{reasoning}\n",
        "\n",
        "Based on this reasoning, provide a refined and expanded search query that would better retrieve relevant information:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        refined_query = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return refined_query\n",
        "\n",
        "    def _generate_answer(self, query: str, reasoning: str, documents: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate final answer based on query, reasoning and documents\"\"\"\n",
        "        # Create context string from documents\n",
        "        context_str = \"\\n\\n\".join([f\"Document {i+1}: {doc['text']}\" for i, doc in enumerate(documents)])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Query: {query}\n",
        "\n",
        "Reasoning process:\n",
        "{reasoning}\n",
        "\n",
        "Retrieved documents:\n",
        "{context_str}\n",
        "\n",
        "Based on the reasoning and documents, provide a comprehensive and accurate answer to the query:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "OYriQGvONeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ReasoningRAGChatbot: Provides the chat interface\n",
        "\n",
        "Tracks conversation history\n",
        "\n",
        "Handles user inputs\n",
        "\n",
        "Returns responses or detailed results"
      ],
      "metadata": {
        "id": "Wpntmfrn_5nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningRAGChatbot:\n",
        "    \"\"\"Chatbot interface for the RAG Reasoning system\"\"\"\n",
        "\n",
        "    def __init__(self, rag_reasoner: RAGReasoner):\n",
        "        \"\"\"Initialize with RAGReasoner\"\"\"\n",
        "        self.rag_reasoner = rag_reasoner\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def chat(self, user_input: str) -> str:\n",
        "        \"\"\"Process user input and generate a response\"\"\"\n",
        "        # Add user input to conversation history\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Process the query\n",
        "        result = self.rag_reasoner.process_query(user_input)\n",
        "\n",
        "        # Format a response message\n",
        "        response = f\"Answer: {result['answer']}\"\n",
        "\n",
        "        # Add response to conversation history\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return response\n",
        "\n",
        "    def get_detailed_response(self, user_input: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process user input and return detailed results including reasoning\"\"\"\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        result = self.rag_reasoner.process_query(user_input)\n",
        "\n",
        "        response = f\"Answer: {result['answer']}\"\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return result\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "\n"
      ],
      "metadata": {
        "id": "u1AEsgqmNhrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prepare Dataset"
      ],
      "metadata": {
        "id": "gYbG26tVHLE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    \"\"\"Utility class for loading datasets\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_json(filepath: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load documents from a JSON file\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def create_documents_from_texts(texts: List[str], metadata: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Create document objects from text strings\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            doc = {\"id\": i, \"text\": text}\n",
        "\n",
        "            if metadata and i < len(metadata):\n",
        "                doc.update(metadata[i])\n",
        "\n",
        "            documents.append(doc)\n",
        "\n",
        "        return documents\n"
      ],
      "metadata": {
        "id": "uugjkhB0Nkul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "5cC38PQVF3L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluation:\n",
        "    \"\"\"Evaluation metrics for the RAG system\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_retrieval(retrieved_docs: List[Dict[str, Any]], relevant_ids: List[int]) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate retrieval performance\"\"\"\n",
        "        retrieved_ids = [doc[\"id\"] for doc in retrieved_docs]\n",
        "\n",
        "        # Calculate precision\n",
        "        if not retrieved_ids:\n",
        "            precision = 0.0\n",
        "        else:\n",
        "            precision = len(set(retrieved_ids) & set(relevant_ids)) / len(retrieved_ids)\n",
        "\n",
        "        # Calculate recall\n",
        "        if not relevant_ids:\n",
        "            recall = 1.0\n",
        "        else:\n",
        "            recall = len(set(retrieved_ids) & set(relevant_ids)) / len(relevant_ids)\n",
        "\n",
        "        # Calculate F1\n",
        "        if precision + recall == 0:\n",
        "            f1 = 0.0\n",
        "        else:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n"
      ],
      "metadata": {
        "id": "4DE_c4jeNoZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main function for Sample texts"
      ],
      "metadata": {
        "id": "A51dDBR0Fskr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the system\"\"\"\n",
        "    # Sample documents\n",
        "    sample_texts = [\"OpenAI's GPT-4 is a multimodal large language model capable of understanding both text and image inputs.\",\n",
        "                    \"Retrieval-Augmented Generation (RAG) enhances chatbot performance by fetching relevant external information in real-time before generating a response.\",\n",
        "                    \"Vector databases like FAISS, Pinecone, and Weaviate are commonly used for storing and retrieving document embeddings in RAG pipelines.\",\n",
        "                    \"Fine-tuning or prompt engineering can help tailor the responses of LLMs to domain-specific use cases, such as legal, healthcare, or customer support.\",\n",
        "                    \"Embedding models like text-embedding-ada-002 from OpenAI or all-MiniLM-L6-v2 from Sentence Transformers are popular for generating vector representations of text.\",\n",
        "                    \"LangChain and LlamaIndex are popular frameworks for building RAG-based applications and connecting LLMs with knowledge sources.\",\n",
        "                    \"Chunking strategies, such as splitting by sentence or paragraph, play a critical role in retrieval accuracy and context relevance.\",\n",
        "                    \"RAG systems can outperform standard chatbots in enterprise environments by reducing hallucinations and increasing factual accuracy.\",\n",
        "                    \"Prompt engineering involves crafting inputs to guide language models toward more accurate, useful, and reliable outputs.\",\n",
        "                    \"Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems.\"\n",
        "                  ]\n",
        "\n",
        "    # Create documents\n",
        "    documents = Dataset.create_documents_from_texts(sample_texts)\n",
        "\n",
        "    # Initialize components\n",
        "    document_store = DocumentStore()\n",
        "    document_store.add_documents(documents)\n",
        "\n",
        "    reasoning_module = ReasoningModule()\n",
        "\n",
        "    rag_reasoner = RAGReasoner(document_store, reasoning_module)\n",
        "\n",
        "    chatbot = ReasoningRAGChatbot(rag_reasoner)\n",
        "\n",
        "    # Demo query\n",
        "    query = \"How does RAG relate to NLP?\"\n",
        "    print(f\"Query: {query}\")\n",
        "\n",
        "    result = chatbot.get_detailed_response(query)\n",
        "    print(\"\\nDetailed Response:\")\n",
        "    print(f\"Reasoning: {result['reasoning']}\")\n",
        "    print(f\"Refined Query: {result['refined_query']}\")\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "\n",
        "    # Simple chat interface\n",
        "    print(\"\\n--- Simple Chat Interface ---\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "EJBxzpD0NtF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "97cb02ccc66d4bac9db08ffd1bce4b39",
            "5169979d2614488185f65a7ed4cf080a",
            "417109c358224cf9932027c834237279",
            "3c8b1f93a65043c986a06797116d0aeb",
            "ddddd1bf798e44afba4dc3b0989099d7",
            "bca6b41d73584523befb8f3ecd70a1fd",
            "3c221ceb1ded4fa09cbda7c526ee99b5",
            "bcdc74164d1746b1a110a6fff09bb218",
            "a48b79b94f324275b9335304fe89cf8d",
            "a580f40b118143e1977dd5c226a29bb9",
            "b9b5698b0f3346a686e278753d870b57"
          ]
        },
        "id": "zbF2kaizNwJz",
        "outputId": "36bce445-a608-45dc-f65e-8279ff13ce39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97cb02ccc66d4bac9db08ffd1bce4b39"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How does RAG relate to NLP?\n",
            "\n",
            "Detailed Response:\n",
            "Reasoning: Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems. Document 3: Vector databases like FAISS, Pinecone, and Weaviate are commonly used for storing and retrieving document embeddings in RAG pipelines. Document 4: RAG systems can outperform standard chatbots in enterprise environments by reducing hallucinations and increasing factual accuracy. Document 5: Chunking strategies, such as splitting by sentence or paragraph, play a critical role in retrieval accuracy and context relevance. Therefore, the final answer is document pre-processing.\n",
            "Refined Query: Document pre-processing, metadata tagging, and semantic search are foundational components for building effective RAG systems\n",
            "Answer: document pre-processing\n",
            "\n",
            "--- Simple Chat Interface ---\n",
            "\n",
            "You: What is the role of RAG?\n",
            "Bot: Answer: Enhances chatbot performance by fetching relevant external information in real-time before generating a response.\n",
            "\n",
            "You: What is the benefit of vector database?\n",
            "Bot: Answer: document embeddings in RAG pipelines\n",
            "\n",
            "You: What is LangChain popular for?\n",
            "Bot: Answer: building RAG - based applications and connecting LLMs with knowledge sources\n",
            "\n",
            "You: What is prompt engineering?\n",
            "Bot: Answer: Prompt engineering involves crafting inputs to guide language models toward more accurate, useful, and reliable outputs.\n",
            "\n",
            "You: What are foundational components of building RAG?\n",
            "Bot: Answer: Document pre-processing, metadata tagging, and semantic search\n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vDSYQjViTAtn"
      }
    }
  ]
}